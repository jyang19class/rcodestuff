---
title: "STOR 320/520 Tutorial on Modeling III"
author: ""
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=F)
options(scipen=999)
library(tidyverse)    #Essential Functions
library(modelr)       #Helpful Functions in Modeling
library(purrr)
library(broom)
DATA=read_csv("AirWaterTemp.csv",col_types=cols()) #River Data
set.seed(216)
```

# Introduction

We will continue our work with daily water temperature and air temperature data observed for `r length(unique(DATA$L))` rivers in Spain. In the preview below, `W` represents the daily maximum water temperature and `A` represents the daily maximum air temperature. The data contains almost a full year of data for each of the `r length(unique(DATA$L))` different rivers.

```{r,echo=F}
head(DATA)
```

Using the data, we seek to identify the best model for predicting the maximum water temperature given the maximum air temperature. Previously, we randomly selected 3 rivers to act as a test set. All models were evaluated based on the randomly selected test set. In this tutorial, we explore approaches that ensure that all data is used for both model training and model testing.

In this tutorial, we apply helpful functions in the `purrr`, `modelr`, and `broom` packages. See the following links for helpful articles on performing cross-validation within the tidyverse: [Link 1](https://sjspielman.github.io/bio5312_fall2017/files/kfold_supplement.pdf), [Link 2](https://drsimonj.svbtle.com/k-fold-cross-validation-with-modelr-and-broom), and [Link 3](https://www.r-bloggers.com/easy-cross-validation-in-r-with-modelr/).


A simple introduction to the broom package can be found [here](https://cran.r-project.org/web/packages/broom/vignettes/broom.html). 



# Part 1: K-Fold CV for Polynomial Model Evaluation

## Cross Validation

- One way to evaluate and compare the performance of different models

```{r fig.align="center", echo=FALSE, out.width = '60%'}
knitr::include_graphics("loocv.png")
```

- Leave-one-out cross-validation
    - repeatedly fit the statistical learning method using training sets that contain $n - 1$ observations, and get the prediction of the excluded observation.
    $$
    CV_{(n)} = \frac{1}{n}\sum_{i=1}^n MSE_i
    $$
    where $MSE_i = (y_i-\hat{y}_i)^2$ based on $x_i$ and model fitted by $\{(x_1,y_1),...,(x_{i-1},y_{i-1}),(x_{i+1},y_{i+1}),...,(x_n,y_n)\}$.
    - there is no randomness in the training/validation set splits; performing LOOCV multiple times will always yield the same results.
    - can also be used in the classification setting by replacing $MSE_i$ with error rate $Err_i = I(y_i\neq \hat{y_i})$
    
- K-fold cross-validation
    ```{r fig.align="center", echo=FALSE, out.width = '60%'}
    knitr::include_graphics("5foldcv.png")
    ```
    - involves randomly dividing the set of observations into k groups of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k-1 folds. The mean squared error, $MSE_1$, is then computed on the observations in the held-out fold. This procedure is repeated k times.
    $$
    CV_{(k)} = \frac{1}{k}\sum_{i=1}^k MSE_i
    $$
    
    
## Chunk 1: Exploratory Figures
```{r}
ggplot(data=DATA) +
  geom_point(aes(x=JULIAN_DAY,y=W,color=A),alpha=0.3) + 
  xlab("Day of Year") + ylab("Max Water Temperature") +
  guides(color=guide_legend(title="Max Air \nTemperature")) +
  theme_minimal()
```

## Chunk 2: Polynomial Fitting

$$W=a+\sum_{i=1}^Ib_iA^i+\sum_{j=1}^Jc_jD^j+\varepsilon$$
```{r}
#fit ploy model with A (up to the order of I=4) and JULIAN_DAY (up to the order of J=3)
polymodel=lm(W~poly(A,4)+poly(JULIAN_DAY,3),data=na.omit(DATA))

#use tidy function from broom
tidy(polymodel)

#use glance function from broom
glance(polymodel)
```

## Chunk 3: Splitting Data for Cross-Validation
```{r}
DATA3=na.omit(DATA) %>% crossv_kfold(10)
DATA3

#Each cell in the train/test column contains a resample object, which is an efficient way of referencing a subset of rows in a data frame (?resample to learn more). 

#to convert the resample object to data frame, use as.data.frame(), e.g., as.data.frame(DATA3$test[[1]])
```

## Chunk 4: Fitted Models and Predictions
```{r,eval=F}
#function train.model.func to specify the model.
#inputs: data - data frame
#        i - Polynomial order for A
#        j - Polynomial order for JULIAN_DAY
#output: mod - lm object
train.model.func=function(data,i,j){
  mod=lm(W~poly(A,i)+poly(JULIAN_DAY,j),data=data)
  return(mod)
}

#check train.model.fun function 
DATA4=DATA3 %>% 
       mutate(tr.model=map(train,train.model.func,i=4,j=3))
DATA4
```

## Chunk 5: Predicted Values and Cross-Validated RMSE

```{r}
#function RMSE.func - calculate root mean squared error
RMSE.func=function(actual,predict){
  mse=mean((actual-predict)^2,na.rm=T)
  rmse=sqrt(mse)
  return(rmse)
}
```

```{r}
#utilize the augment function in broom
#Augment accepts a model object and a dataset and adds information about each observation in the dataset. Most commonly, this includes predicted values in the .fitted column, residuals in the .resid column, and standard errors for the fitted values in a .se.fit column.


DATA4.PREDICT = DATA4 %>% 
          mutate(predict=map2(test,tr.model,~augment(.y,newdata=.x))) %>%
          select(predict) %>%
          unnest(cols = c(predict))

#Nesting creates a list-column of data frames; unnesting flattens it back out into regular columns.

head(DATA4.PREDICT)
RMSE.func(actual=DATA4.PREDICT$W,predict=DATA4.PREDICT$.fitted)
```

## Chunk 6: Select Best (i,j) Pair

```{r}
max_i = 10
max_j = 10
rmse_results = matrix(NA,max_i,max_j)

for(i in 1:max_i){
  for(j in 1:max_j){
    
    #train data on training 
    DATA4=DATA3 %>% 
       mutate(tr.model=map(train,train.model.func,i=i,j=j))
    
    #do prediction
    DATA4.PREDICT = DATA4 %>% 
          mutate(predict=map2(test,tr.model,~augment(.y,newdata=.x))) %>%
          select(predict) %>%
          unnest()
    
    #calculate rmse
    rmse_results[i,j] = RMSE.func(actual=DATA4.PREDICT$W,predict=DATA4.PREDICT$.fitted)
  }
}
```

```{r}
#visualize the rmse_results

colnames(rmse_results) <- paste("i", 1:10, sep="")
rownames(rmse_results) <- paste("j", 1:10, sep="")
m = rmse_results

image(1:ncol(m), 1:nrow(m), t(m), col = terrain.colors(60), axes = FALSE)
axis(1, 1:ncol(m), colnames(m))
axis(2, 1:nrow(m), rownames(m))
for (x in 1:ncol(m))
  for (y in 1:nrow(m))
    text(x, y, format(m[y,x], digits=3))

#which one is the best?
which(rmse_results==min(rmse_results), arr.ind = T)
```

# Part 2: Exercise on Simulated Data

1. We will now perform cross-validation on a simulated data set.

(a) Generate a simulated data set as follows:
n=100 and p=2, the model used is
$$
Y = X - 2X^2 + \epsilon
$$
$\epsilon$ here follow standard normal distribution. 

```{r, eval=F}
set.seed(123)
x=rnorm(100)
y=x -  2*x^2 + rnorm(100)
sim=tibble(predictor=x,response=y)
```


(b) Create a scatterplot of X against Y. Comment on what you find.
```{r, eval=F}
plot(x,y)
```

Comment: ANSWER_HERE.

(c) Compute the 5-Fold RMSE that result from fitting the following four models:

i. $Y = \beta_0 + \beta_1X + \epsilon$

ii. $Y = \beta_0 + \beta_1X + \beta_2X^2 + \epsilon$

iii. $Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon$

iv. $Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \beta_4X^4 + \epsilon$

```{r, eval=F}
max_i = 4
rmse_ex = rep(NA,max_i)
train.model.func2 = function(data,i){
  mod = lm(response~poly(predictor, i), data=data)
  return(mod)
}
for(i in 1:max_i){
  sim_final = sim %>%
    crossv_kfold(k=5) %>%
    mutate(tr.model=map(train,train.model.func2,i=i)) %>%
    mutate(predict=map2(test,tr.model,~augment(.y,newdata=.x))) %>%
    select(predict) %>%
    unnest()
  rmse_ex[i] = RMSE.func(actual=sim_final$response,predict=sim_final$.fitted)
}
```

Check which model has the smallest RMSE:
```{r,eval=F}
rmse_ex
```

(d) Which of the models in (c) had the smallest RMSE? Is
this what you expected? Explain your answer.

ANSWER_HERE
